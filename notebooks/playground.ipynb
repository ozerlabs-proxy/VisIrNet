{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import json\n",
    "\n",
    "#change working directory to root\n",
    "ROOT_DIR = os.getcwd()\n",
    "while os.path.basename(ROOT_DIR) != 'VisIrNet':\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(ROOT_DIR,'..'))\n",
    "sys.path.insert(0,ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "ROOT_DIR = Path(ROOT_DIR)\n",
    "\n",
    "print(tf.__version__)\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"len(devices): \", len(devices))\n",
    "print(f\"available GPUs: {devices}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file to load will be passed as an argument\n",
    "# get run parameters\n",
    "\n",
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config-file', \n",
    "                        action = \"store\", \n",
    "                        dest = \"config_file\",\n",
    "                        default = \"default_config.json\",\n",
    "                        help = 'specify config file to load')\n",
    "\n",
    "input_arguments = parser.parse_args([])\n",
    "\n",
    "from Tools.configurations_parser import ConfigurationParser\n",
    "# load configurations\n",
    "configs = ConfigurationParser.getConfigurations(configs_path = 'configs', \n",
    "                                                config_file = str(input_arguments.config_file))\n",
    "\n",
    "\n",
    "# print configurations\n",
    "ConfigurationParser.printConfigurations(configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_setup\n",
    "\n",
    "train_dataloader,test_dataloader = data_setup.create_dataloaders(dataset=configs.dataset, \n",
    "                                                                BATCH_SIZE=configs.BATCH_SIZE,\n",
    "                                                                SHUFFLE_BUFFER_SIZE=configs.SHUFFLE_BUFFER_SIZE\n",
    "                                                                )\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_setup\n",
    "import Utils\n",
    "\n",
    "featureEmbeddingBackBone = model_setup.getFeatureEmbeddingBackBone(rgb_inputs_shape=configs.RGB_INPUTS_SHAPE,\n",
    "                                                        ir_inputs_shape=configs.IR_INPUTS_SHAPE,\n",
    "                                                        output_channels_per_block=configs.OUTPUT_CHANNELS_PER_BLOCK\n",
    "                                                        )\n",
    "\n",
    "regressionHead= model_setup.getRegressionHead(input_shape=configs.REGRESSION_INPUT_SHAPE,\n",
    "                                                output_size=configs.REGRESSION_OUTPUT_SHAPE\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize and save model structures**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "# visualize and save models\n",
    "\n",
    "Utils.plot_and_save_model_structure(featureEmbeddingBackBone,\n",
    "                                            save_path=\"resources/\",\n",
    "                                            save_as=f\"featureEmbeddingBackBone\")\n",
    "Utils.plot_and_save_model_structure(regressionHead,\n",
    "                                            save_path=\"resources/\",\n",
    "                                            save_as=f\"regressionHead\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine \n",
    "\n",
    "if configs.TrainFistStage:\n",
    "    print(\"*\"*25, f\"first stage\", \"*\"*25)\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=configs.B_initial_learning_rate,\n",
    "                                                                    decay_steps=configs.B_decay_steps,\n",
    "                                                                    decay_rate=configs.B_decay_rate,\n",
    "                                                                    staircase=True)\n",
    "\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # Start the timer\n",
    "    from timeit import default_timer as timer\n",
    "    start_time = timer()\n",
    "    # Train model \n",
    "\n",
    "    model_results = engine.train_first_stage(model=featureEmbeddingBackBone,\n",
    "                                                    train_dataloader=train_dataloader,\n",
    "                                                    test_dataloader=test_dataloader,\n",
    "                                                    dataset_name=configs.dataset,\n",
    "                                                    optimizer=optimizer,\n",
    "                                                    epochs=configs.B_NUM_EPOCHS,\n",
    "                                                    from_checkpoint=configs.B_from_checkpoint,\n",
    "                                                    save_path=configs.B_save_path,\n",
    "                                                    save_as=configs.B_save_as,\n",
    "                                                    save_frequency=configs.B_save_frequency,\n",
    "                                                    save_hard_frequency=configs.B_save_hard_frequency,\n",
    "                                                    )\n",
    "    # End the timer and print out how long it took\n",
    "    end_time = timer()\n",
    "    print(f\"Total training time : {end_time-start_time:.3f} seconds\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**second stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine \n",
    "\n",
    "if configs.TrainSecondStage:\n",
    "    print(\"*\"*25, f\"second stage\", \"*\"*25)\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=configs.R_initial_learning_rate,\n",
    "                                                                    decay_steps=configs.R_decay_steps,\n",
    "                                                                    decay_rate=configs.R_decay_rate,\n",
    "                                                                    staircase=True)\n",
    "\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # Start the timer\n",
    "    from timeit import default_timer as timer\n",
    "    start_time = timer()\n",
    "    # Train model \n",
    "\n",
    "    model_results = engine.train_second_stage(model = regressionHead,\n",
    "                                            featureEmbeddingBackBone = configs.R_featureEmbeddingBackBone,\n",
    "                                            train_dataloader = train_dataloader,\n",
    "                                            test_dataloader = test_dataloader,\n",
    "                                            dataset_name = configs.dataset,\n",
    "                                            optimizer = optimizer,\n",
    "                                            epochs = configs.R_NUM_EPOCHS,\n",
    "                                            from_checkpoint = configs.R_from_checkpoint,\n",
    "                                            save_path = configs.R_save_path,\n",
    "                                            save_as = configs.R_save_as,\n",
    "                                            save_frequency = configs.R_save_frequency,\n",
    "                                            save_hard_frequency = configs.R_save_hard_frequency,\n",
    "                                            predicting_homography = configs.R_predicting_homography\n",
    "                                            )\n",
    "    # End the timer and print out how long it took\n",
    "    end_time = timer()\n",
    "    print(f\"Total training time : {end_time-start_time:.3f} seconds\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisIrNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

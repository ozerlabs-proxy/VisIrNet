{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import json\n",
    "\n",
    "#change working directory to root\n",
    "ROOT_DIR = os.getcwd()\n",
    "while os.path.basename(ROOT_DIR) != 'VisIrNet':\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(ROOT_DIR,'..'))\n",
    "sys.path.insert(0,ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "ROOT_DIR = Path(ROOT_DIR)\n",
    "\n",
    "print(tf.__version__)\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"len(devices): \", len(devices))\n",
    "print(f\"available GPUs: {devices}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_setup\n",
    "\n",
    "# try to import the dataset\n",
    "dataset=\"SkyData\"\n",
    "BATCH_SIZE = 2\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataloader,test_dataloader = data_setup.create_dataloaders(dataset=dataset, \n",
    "                                                                BATCH_SIZE=BATCH_SIZE,\n",
    "                                                                SHUFFLE_BUFFER_SIZE=100\n",
    "                                                                )\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_setup\n",
    "import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**configuration**\n",
    "\n",
    "\n",
    "or load configuration from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config constants \n",
    "configs = {\n",
    "            'RGB_INPUTS_SHAPE' : (192,192,3),\n",
    "            'IR_INPUTS_SHAPE' :  (128,128,3),\n",
    "            'OUTPUT_CHANNELS_PER_BLOCK' : 3,\n",
    "            'REGRESSION_INPUT_SHAPE' : None,\n",
    "            'REGRESSION_OUTPUT_SHAPE' : 8    \n",
    "            }\n",
    "configs['REGRESSION_INPUT_SHAPE']= (*configs[\"RGB_INPUTS_SHAPE\"][:2], configs[\"OUTPUT_CHANNELS_PER_BLOCK\"]*2)\n",
    "assert configs['REGRESSION_INPUT_SHAPE'] != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureEmbeddingBackBone = model_setup.getFeatureEmbeddingBackBone(rgb_inputs_shape=configs['RGB_INPUTS_SHAPE'],\n",
    "                                                        ir_inputs_shape=configs['IR_INPUTS_SHAPE'],\n",
    "                                                        output_channels_per_block=configs['OUTPUT_CHANNELS_PER_BLOCK']\n",
    "                                                        )\n",
    "\n",
    "regressionHead= model_setup.getRegressionHead(input_shape=configs['REGRESSION_INPUT_SHAPE'],\n",
    "                                                output_size=configs['REGRESSION_OUTPUT_SHAPE']\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize and save model structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize and save models\n",
    "\n",
    "# Utils.plot_and_save_model_structure(featureEmbeddingBackBone,\n",
    "#                                             save_path=\"resources/\",\n",
    "#                                             save_as=f\"featureEmbeddingBackBone\")\n",
    "# Utils.plot_and_save_model_structure(regressionHead,\n",
    "#                                             save_path=\"resources/\",\n",
    "#                                             save_as=f\"regressionHead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import engine \n",
    "\n",
    "# initial_learning_rate = 0.001\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "#                                                                 decay_steps=10000,\n",
    "#                                                                 decay_rate=0.96,\n",
    "#                                                                 staircase=True)\n",
    "# NUM_EPOCHS = 5\n",
    "\n",
    "# # Setup optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# # Start the timer\n",
    "# from timeit import default_timer as timer\n",
    "# start_time = timer()\n",
    "# # Train model \n",
    "\n",
    "# model_results = engine.train_first_stage(model=featureEmbeddingBackBone,\n",
    "#                                                 train_dataloader=train_dataloader,\n",
    "#                                                 test_dataloader=test_dataloader,\n",
    "#                                                 optimizer=optimizer,\n",
    "#                                                 epochs=NUM_EPOCHS,\n",
    "#                                                 from_checkpoint=\"latest\",\n",
    "#                                                 save_path=\"models\",\n",
    "#                                                 save_as=f\"featureEmbeddingBackBone\",\n",
    "#                                                 save_frequency=1,\n",
    "#                                                 save_hard_frequency=50\n",
    "#                                                 )\n",
    "# # End the timer and print out how long it took\n",
    "# end_time = timer()\n",
    "# print(f\"Total training time : {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**second stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine \n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                                decay_steps=10000,\n",
    "                                                                decay_rate=0.96,\n",
    "                                                                staircase=True)\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "# Train model \n",
    "\n",
    "model_results = engine.train_second_stage(model=regressionHead,\n",
    "                                        featureEmbeddingBackBone=\"latest\",\n",
    "                                        train_dataloader=train_dataloader,\n",
    "                                        test_dataloader=test_dataloader,\n",
    "                                        optimizer=optimizer,\n",
    "                                        epochs=NUM_EPOCHS,\n",
    "                                        from_checkpoint=\"latest\",\n",
    "                                        save_path=\"models\",\n",
    "                                        save_as=f\"regressionHead\",\n",
    "                                        save_frequency=1,\n",
    "                                        save_hard_frequency=20\n",
    "                                        )\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time : {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 17:13:24.780272: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-09 17:13:24.814875: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-09 17:13:24.814905: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-09 17:13:24.814926: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-09 17:13:24.820420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 5, 5, 3]), TensorShape([1, 2, 2, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.ones([1,192,192,3])\n",
    "t2 = tf.ones([1,128,128,3])\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_height = (t.shape[1] - t2.shape[1]) // 2\n",
    "pad_width = (t.shape[2] - t2.shape[2]) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [1, 1], [1, 1], [0, 0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddings = [[0,0], [pad_height, pad_height], [pad_width, pad_width], [0,0]]\n",
    "paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=\n",
       "array([[[[ 0.46643844, -1.1350483 , -1.5372299 ],\n",
       "         [-0.49728367, -0.50578505,  0.38425392]],\n",
       "\n",
       "        [[-0.6790849 ,  1.1726139 , -0.7692692 ],\n",
       "         [ 0.7509908 , -0.43396217,  0.7430156 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4, 3), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.46643844, -1.1350483 , -1.5372299 ],\n",
       "        [-0.49728367, -0.50578505,  0.38425392],\n",
       "        [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ],\n",
       "        [-0.6790849 ,  1.1726139 , -0.7692692 ],\n",
       "        [ 0.7509908 , -0.43396217,  0.7430156 ],\n",
       "        [ 0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out = tf.pad(t2, paddings, 'CONSTANT', constant_values=0)\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisIrNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

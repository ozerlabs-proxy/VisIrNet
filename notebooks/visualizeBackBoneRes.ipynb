{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#change working directory to root\n",
    "ROOT_DIR = os.getcwd()\n",
    "while os.path.basename(ROOT_DIR) != 'VisIrNet':\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(ROOT_DIR,'..'))\n",
    "sys.path.insert(0,ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "ROOT_DIR = Path(ROOT_DIR)\n",
    "\n",
    "print(tf.__version__)\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"len(devices): \", len(devices))\n",
    "print(f\"available GPUs: {devices}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "## gpu setup \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] reading configurations from configs/vedai_default_config.json\n"
     ]
    }
   ],
   "source": [
    "# config file to load will be passed as an argument\n",
    "# get run parameters\n",
    "\n",
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config-file', \n",
    "                        action = \"store\", \n",
    "                        dest = \"config_file\",\n",
    "                        default = \"vedai_default_config.json\",\n",
    "                        help = 'specify config file to load')\n",
    "\n",
    "input_arguments = parser.parse_args([])\n",
    "\n",
    "from Tools.configurations_parser import ConfigurationParser\n",
    "# load configurations\n",
    "configs = ConfigurationParser.getConfigurations(configs_path = 'configs', \n",
    "                                                config_file = str(input_arguments.config_file))\n",
    "\n",
    "\n",
    "# print configurations\n",
    "# ConfigurationParser.printConfigurations(configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading train dataset\n",
      "[INFO] train _dataset:  8722\n",
      "[INFO] loading val dataset\n",
      "[INFO] val _dataset:  3738\n",
      "dataset: VEDAI\n",
      "BATCH_SIZE: 2\n",
      "SHUFFLE_BUFFER_SIZE: 1000\n",
      "train_dataloader: 4361\n",
      "test_dataloader: 1869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4361, 1869)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_setup\n",
    "\n",
    "train_dataloader,test_dataloader = data_setup.create_dataloaders(dataset=configs.dataset, \n",
    "                                                                BATCH_SIZE=configs.BATCH_SIZE,\n",
    "                                                                SHUFFLE_BUFFER_SIZE=configs.SHUFFLE_BUFFER_SIZE\n",
    "                                                                )\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model from models/VEDAI/featureEmbeddingBackBone-5827618d1f2048e0819760780721fcbc-7.keras\n"
     ]
    }
   ],
   "source": [
    "## model name to load\n",
    "import model_setup\n",
    "import Utils\n",
    "\n",
    "\n",
    "import Tools.utilities as common_utils\n",
    "\n",
    "from_checkpoint = \"featureEmbeddingBackBone-5827618d1f2048e0819760780721fcbc-7.keras\"\n",
    "save_path = \"models/VEDAI\"\n",
    "\n",
    "pattern = f\"*featureEmbeddingBackBone*\" if str(from_checkpoint)==\"latest\" else f\"{from_checkpoint}*\"\n",
    "\n",
    "try:\n",
    "    model_name = common_utils.latest_file(Path(save_path), pattern=pattern)\n",
    "    model = common_utils.load_model(model_name)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"error loading model\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## \n",
    "\n",
    "def plot_showcase_and_save_backbone_results( all_data_to_plot,\n",
    "                                        _instances,\n",
    "                                        save_path = \"resources/backbone-showcase\",\n",
    "                                        dataset_name = \"VEDAI\"\n",
    "                                        ):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # input_images = all_data_to_plot[0]\n",
    "    template_images = all_data_to_plot[1]\n",
    "    warped_inputs = all_data_to_plot[2]\n",
    "    # rgb_fmaps = all_data_to_plot[4]\n",
    "    ir_fmaps = all_data_to_plot[5]\n",
    "    warped_fmaps = all_data_to_plot[6]\n",
    "\n",
    "    for i_th in range(len(all_data_to_plot[0])):\n",
    "        data_to_plot = {k:np.array(v[i_th]).clip(0,1) for k,v in all_data_to_plot.items()}\n",
    "        \n",
    "        summed_data = {\n",
    "                3: 0.5 *( warped_inputs[i_th] + warped_fmaps[i_th]),\n",
    "                7: 0.5 * (template_images[i_th] + tf.cast(ir_fmaps[i_th],tf.float32))\n",
    "        }\n",
    "        data_to_plot.update(summed_data)\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 4, figsize=(8, 5), constrained_layout=True)\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        # fig = plt.figure(figsize=(20, 20))\n",
    "        # axs = fig.subplots(3, 2)\n",
    "        titles=[\"input_images\",\"template_images\",\"warped_inputs\",\"summed_rgb\",\"rgb_fmaps\",\"ir_fmaps\",\"warped_fmaps\",\"summed_ir\"]\n",
    "        for i, ax in enumerate(axs):\n",
    "            ax.axis('off')\n",
    "            ax.set_title(titles[i])\n",
    "            \n",
    "\n",
    "\n",
    "        for i, data_i in data_to_plot.items():\n",
    "            axs[i].imshow(np.array(data_i).clip(0,1))\n",
    "        # fig.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        #saving showcase\n",
    "        \n",
    "        save_path = Path(f\"{save_path}/{dataset_name}/\")\n",
    "        save_name = f\"{_instances.numpy()[i_th].decode('utf-8')}.png\"\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        save_as= str(save_path/save_name)\n",
    "        \n",
    "        plt.savefig(save_as, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Forward Pass**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_visualize = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Tools.loss_functions as loss_functions\n",
    "import Tools.datasetTools as DatasetTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_visualize = samples_to_visualize // configs.BATCH_SIZE\n",
    "\n",
    "for batch in train_dataloader.take(samples_to_visualize):\n",
    "    input_images, template_images, labels,_instances = batch\n",
    "    \n",
    "    gt_matrix = DatasetTools.get_ground_truth_homographies(labels)\n",
    "    warped_inputs, _ = DatasetTools._get_warped_sampled(images = input_images,  homography_matrices = gt_matrix)\n",
    "    \n",
    "    rgb_fmaps , ir_fmaps = model.call((input_images, template_images), training=False)\n",
    "    \n",
    "    \n",
    "    warped_fmaps,_ = DatasetTools._get_warped_sampled( images = rgb_fmaps, \n",
    "                                                    homography_matrices = gt_matrix)\n",
    "\n",
    "\n",
    "    total_loss , detailed_batch_losses = loss_functions.get_losses_febackbone( warped_inputs,\n",
    "                                                                                template_images,\n",
    "                                                                                warped_fmaps,\n",
    "                                                                                ir_fmaps)\n",
    "\n",
    "\n",
    "\n",
    "    all_data_to_plot = {\n",
    "                        0:input_images,\n",
    "                        1:template_images,\n",
    "                        2:warped_inputs,\n",
    "                        4:rgb_fmaps,\n",
    "                        5:ir_fmaps,\n",
    "                        6:warped_fmaps,\n",
    "                                    }\n",
    "    \n",
    "    plot_showcase_and_save_backbone_results( all_data_to_plot,\n",
    "                                            _instances,\n",
    "                                            save_path = \"resources/backbone-showcase\",\n",
    "                                            dataset_name = configs.dataset\n",
    "                                            )\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisIrNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n",
      "len(devices):  1\n",
      "available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#change working directory to root\n",
    "ROOT_DIR = os.getcwd()\n",
    "while os.path.basename(ROOT_DIR) != 'VisIrNet':\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(ROOT_DIR,'..'))\n",
    "sys.path.insert(0,ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "ROOT_DIR = Path(ROOT_DIR)\n",
    "\n",
    "print(tf.__version__)\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"len(devices): \", len(devices))\n",
    "print(f\"available GPUs: {devices}\");\n",
    "\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract important metrics from logs\n",
    "\n",
    "logs_path = ROOT_DIR / 'logs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backbone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## helper functions\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions\n",
    "\n",
    "def parse_backbone_log(logs):\n",
    "    \"\"\"\n",
    "        'model-name',\n",
    "        'epochs',\n",
    "        'loss_function',\n",
    "        'resumed_from',\n",
    "        'train_size',\n",
    "        'test_size',\n",
    "        'tag_name',\n",
    "        'per_epoch_metrics',\n",
    "        'training_time'\n",
    "    \"\"\"\n",
    "    #\n",
    "    # interested_in = ['fir_frgb', 'frgb_Irgb', 'fir_Irgb']\n",
    "    interested_in = ['total_loss']\n",
    "\n",
    "\n",
    "    # exatract the initial and final values\n",
    "    training_logs = logs[\"per_epoch_metrics\"][\"train_loss\"]\n",
    "    test_logs = logs[\"per_epoch_metrics\"][\"test_results\"]\n",
    "    \n",
    "    _filtered_logs={}\n",
    "\n",
    "    _filtered_logs.update({f\"Train\": v for k,v in training_logs.items() if k in interested_in})\n",
    "    _filtered_logs.update({f\"Test\": v for k,v in test_logs.items() if k in interested_in})\n",
    "    \n",
    "    values_dict = {logs[\"loss_function\"]: _filtered_logs}\n",
    "    return dict(values_dict)\n",
    "\n",
    "\n",
    "def handle_backbone(log_dir, backbone_logs):\n",
    "    \n",
    "    backbone_parsed_dict = defaultdict(list)\n",
    "    \n",
    "    for b_logs in backbone_logs:        \n",
    "        # read the json \n",
    "        with open(log_dir / b_logs) as f:\n",
    "            example_logs = json.load(f)\n",
    "            parsed_dict = parse_backbone_log(example_logs)\n",
    "        \n",
    "        # add the parsed dict to the backbone_parsed_dict\n",
    "        backbone_parsed_dict.update(parsed_dict)\n",
    "            \n",
    "    return dict(backbone_parsed_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plts\n",
    "\n",
    "def handle_learning_curve_plots(history_dict,d_name):\n",
    "    \"\"\"\n",
    "    \n",
    "        Given training history dict for different loss functions,\n",
    "        plot the training and validation loss and accuracy curves\n",
    "    \"\"\"\n",
    "    print(\"dataset_name: \", d_name)\n",
    "    print(\"*\"*50)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle(f'{d_name} Learning Curves')\n",
    "    \n",
    "    titles_dict = {\n",
    "        \"sse_pixel\": \"Pixel Sum Squared Error\",\n",
    "        \"mae_pixel\": \"Pixel Mean Absolute Error\",\n",
    "        \"mse_pixel\": \"Pixel Mean Squared Error\",\n",
    "        \"ssim_pixel\": \"Pixel Structural Similarity Index\"\n",
    "    }\n",
    "    \n",
    "    raveled_axes = axes.ravel() \n",
    "            \n",
    "    for idx, (k , v) in enumerate(history_dict.items()):\n",
    "        print(f\"loss: {k}\")\n",
    "        _logs_df = pd.DataFrame(v)\n",
    "        with sns.axes_style(\"whitegrid\"):\n",
    "            sns.lineplot(data= _logs_df, ax=raveled_axes[idx],linewidth=2.5)\n",
    "            raveled_axes[idx].set_title(f'{titles_dict[k]}')\n",
    "            raveled_axes[idx].set_xlabel('' if idx <2  else \"Epochs\")\n",
    "            raveled_axes[idx].set_ylabel('')\n",
    "            raveled_axes[idx].tick_params(axis='x', labelbottom=(True if idx > 1 else False))\n",
    "            raveled_axes[idx].tick_params(axis='y', labelleft=True)\n",
    "            # axes[i, j].legend(ncol=1, fontsize=5, loc=\"upper center\", frameon=False).remove()      \n",
    "        # # show ticks for the last row\n",
    "    # Adjust spacing\n",
    "    plt.tight_layout()\n",
    "    plt.fontsize = 20\n",
    "    # Show the plots\n",
    "    # plt.show()    \n",
    "    file_name = f'{d_name}_learning_curve'\n",
    "    save_path = Path(\"resources/learning_curve_s/\")\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(str(save_path/f'{file_name}.png'), dpi=300, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    print(\"*\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:  VEDAI\n",
      "**************************************************\n",
      "loss: mae_pixel\n",
      "loss: mse_pixel\n",
      "loss: sse_pixel\n",
      "loss: ssim_pixel\n",
      "**************************************************\n",
      "dataset_name:  SkyData\n",
      "**************************************************\n",
      "loss: sse_pixel\n",
      "loss: ssim_pixel\n",
      "loss: mse_pixel\n",
      "loss: mae_pixel\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get all directories in logs_path\n",
    "log_dirs = [x for x in logs_path.iterdir() if x.is_dir() and x.name != 'tensorboard']\n",
    "dataset_with_logs = [d.name for d in log_dirs]\n",
    "\n",
    "\n",
    "\n",
    "for log_dir, d_name in zip(log_dirs,dataset_with_logs):\n",
    "    # per dataset logs\n",
    "    per_dataset_logs = [x.name for x in log_dir.glob('*.json')]\n",
    "\n",
    "    backbone_logs = list(filter(lambda x: 'regressionBlock' not in x,per_dataset_logs))\n",
    "    backbone_parsed = handle_backbone(log_dir, backbone_logs)\n",
    "    # regression_parsed = handle_regression(log_dir, regressionBlock_logs)\n",
    "    \n",
    "    handle_learning_curve_plots(backbone_parsed,d_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisIrNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

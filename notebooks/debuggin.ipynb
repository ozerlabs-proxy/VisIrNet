{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n",
      "len(devices):  1\n",
      "available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nalain/miniconda3/envs/VisIrNet/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#change working directory to root\n",
    "ROOT_DIR = os.getcwd()\n",
    "while os.path.basename(ROOT_DIR) != 'VisIrNet':\n",
    "    ROOT_DIR = os.path.abspath(os.path.join(ROOT_DIR,'..'))\n",
    "sys.path.insert(0,ROOT_DIR)\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "ROOT_DIR = Path(ROOT_DIR)\n",
    "\n",
    "print(tf.__version__)\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"len(devices): \", len(devices))\n",
    "print(f\"available GPUs: {devices}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "## gpu setup \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] reading configurations from configs/vedai_default_config.json\n",
      "************************* Configurations *************************\n",
      "\t dataset: VEDAI\n",
      "\t TrainFirstStage: True\n",
      "\t TrainSecondStage: True\n",
      "\t B_R_uuid: 61a2b4810a9f4e9ba7a084652436f0ea\n",
      "\t BATCH_SIZE: 2\n",
      "\t SHUFFLE_BUFFER_SIZE: 1000\n",
      "\t RGB_INPUTS_SHAPE: [192, 192, 3]\n",
      "\t IR_INPUTS_SHAPE: [128, 128, 3]\n",
      "\t B_STACK_COUNT: 1\n",
      "\t R_STACK_COUNT: 1\n",
      "\t OUTPUT_CHANNELS_PER_BLOCK: 3\n",
      "\t REGRESSION_INPUT_SHAPE: [192, 192, 6]\n",
      "\t REGRESSION_OUTPUT_SHAPE: 8\n",
      "\t B_initial_learning_rate: 0.01\n",
      "\t B_decay_steps: 1000\n",
      "\t B_decay_rate: 0.96\n",
      "\t B_NUM_EPOCHS: 2\n",
      "\t B_from_checkpoint: None\n",
      "\t B_save_path: models/VEDAI\n",
      "\t B_save_as: featureEmbeddingBackBone\n",
      "\t B_save_frequency: 1\n",
      "\t B_save_hard_frequency: 20\n",
      "\t R_initial_learning_rate: 0.001\n",
      "\t R_decay_steps: 1000\n",
      "\t R_decay_rate: 0.96\n",
      "\t R_NUM_EPOCHS: 2\n",
      "\t R_featureEmbeddingBackBone: latest\n",
      "\t R_from_checkpoint: None\n",
      "\t R_save_path: models/VEDAI\n",
      "\t R_save_as: regressionHead\n",
      "\t R_save_frequency: 1\n",
      "\t R_save_hard_frequency: 10\n",
      "\t R_predicting_homography: True\n",
      "*****************************************************************\n"
     ]
    }
   ],
   "source": [
    "# config file to load will be passed as an argument\n",
    "# get run parameters\n",
    "\n",
    "import argparse \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config-file', \n",
    "                        action = \"store\", \n",
    "                        dest = \"config_file\",\n",
    "                        default = \"vedai_default_config.json\",\n",
    "                        help = 'specify config file to load')\n",
    "\n",
    "input_arguments = parser.parse_args([])\n",
    "\n",
    "from Tools.configurations_parser import ConfigurationParser\n",
    "# load configurations\n",
    "configs = ConfigurationParser.getConfigurations(configs_path = 'configs', \n",
    "                                                config_file = str(input_arguments.config_file))\n",
    "\n",
    "\n",
    "# print configurations\n",
    "ConfigurationParser.printConfigurations(configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading train dataset\n",
      "[INFO] train _dataset:  8722\n",
      "[INFO] loading val dataset\n",
      "[INFO] val _dataset:  3738\n",
      "dataset: VEDAI\n",
      "BATCH_SIZE: 2\n",
      "SHUFFLE_BUFFER_SIZE: 1000\n",
      "train_dataloader: 4361\n",
      "test_dataloader: 1869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4361, 1869)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_setup\n",
    "\n",
    "train_dataloader,test_dataloader = data_setup.create_dataloaders(dataset=configs.dataset, \n",
    "                                                                BATCH_SIZE=configs.BATCH_SIZE,\n",
    "                                                                SHUFFLE_BUFFER_SIZE=configs.SHUFFLE_BUFFER_SIZE\n",
    "                                                                )\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_setup\n",
    "import Utils\n",
    "\n",
    "featureEmbeddingBackBone = model_setup.getFeatureEmbeddingBackBone(rgb_inputs_shape=configs.RGB_INPUTS_SHAPE,\n",
    "                                                        ir_inputs_shape=configs.IR_INPUTS_SHAPE,\n",
    "                                                        output_channels_per_block=configs.OUTPUT_CHANNELS_PER_BLOCK,\n",
    "                                                        blocks_count=configs.B_STACK_COUNT,\n",
    "                                                        )\n",
    "\n",
    "# regressionHead= model_setup.getRegressionHead(input_shape=configs.REGRESSION_INPUT_SHAPE,\n",
    "#                                                 output_size=configs.REGRESSION_OUTPUT_SHAPE,\n",
    "#                                                 blocks_count=configs.R_STACK_COUNT,\n",
    "#                                                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize and save model structures**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=configs.B_initial_learning_rate,\n",
    "                                                                decay_steps=configs.B_decay_steps,\n",
    "                                                                decay_rate=configs.B_decay_rate,\n",
    "                                                                staircase=True)\n",
    "\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = featureEmbeddingBackBone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Tools.loss_functions as loss_functions\n",
    "import Tools.datasetTools as DatasetTools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f44a02e3640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f44a02e3640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:07<00:00, 63.73s/it]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "for epoch in tqdm(range(2)):\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader.take(256)):\n",
    "        \n",
    "        input_images, template_images, labels,_instances = batch\n",
    "        \n",
    "        tf.config.run_functions_eagerly(True)\n",
    "        gt_matrix = DatasetTools.get_ground_truth_homographies(labels)\n",
    "        tf.config.run_functions_eagerly(False)\n",
    "        \n",
    "        warped_inputs, _ = DatasetTools._get_warped_sampled(images = input_images,  homography_matrices = gt_matrix)\n",
    "\n",
    "        ###    warped_inputs\n",
    "\n",
    "        # warped_inputs.shape, tf.reduce_max(warped_inputs), tf.reduce_min(warped_inputs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "                        #persistent=True\n",
    "                        rgb_fmaps , ir_fmaps = model.call((input_images, template_images), training=True)\n",
    "                        # total_loss = tf.constant(0.0)\n",
    "                        tape.watch(rgb_fmaps)\n",
    "                        tape.watch(ir_fmaps)\n",
    "                        \n",
    "                        \n",
    "                        warped_fmaps,_ = DatasetTools._get_warped_sampled( images = rgb_fmaps, \n",
    "                                                                            homography_matrices = gt_matrix)\n",
    "                        \n",
    "                        tape.watch(warped_fmaps)\n",
    "                        \n",
    "                                        \n",
    "                        total_loss , detailed_batch_losses = loss_functions.get_losses_febackbone( warped_inputs,\n",
    "                                                                                                    template_images,\n",
    "                                                                                                    warped_fmaps,\n",
    "                                                                                                    ir_fmaps)\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "        # get gradients and backpropagate                \n",
    "        all_parameters= model.trainable_variables\n",
    "        grads = tape.gradient(total_loss, all_parameters)#,unconnected_gradients=tf.UnconnectedGradients.ZERO\n",
    "\n",
    "        grads_are_safe = np.array([ tf.math.is_finite(g).numpy().all() for g in grads ]).all()\n",
    "        \n",
    "        assert grads_are_safe, F\"Gradients are not safe at iteration: {i}\"\n",
    "        #backpropagate\n",
    "\n",
    "        grads = [tf.clip_by_value(i,-0.1,0.1) for i in grads] \n",
    "        optimizer.apply_gradients(zip(grads, all_parameters))\n",
    "        \n",
    "        detailed_batch_losses = {str(i): k.numpy() for i, k in detailed_batch_losses.items()}\n",
    "        loss_message = \" | \".join([str(str(i)+ \" : \" + str(k)) for i,k in detailed_batch_losses.items()])\n",
    "                \n",
    "        # print(f\"{i}:: shape: {warped_fmaps.shape} | max: {tf.reduce_max(warped_fmaps)} | min: {tf.reduce_min(warped_fmaps)}| grads_are_safe: {grads_are_safe} --- loss: {total_loss.numpy()}| {loss_message}\")\n",
    "        # print(f\"{i}:: shape: {warped_fmaps.shape} | max: {tf.reduce_max(warped_fmaps)} | min: {tf.reduce_min(warped_fmaps)}| grads_are_safe: {grads_are_safe} --- loss: {total_loss.numpy()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.07401132>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisIrNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
